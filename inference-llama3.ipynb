{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "from transformers import TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "base_model = \"unsloth/meta-llama-3.1-8b-instruct-bnb-4bit\"\n",
    "\n",
    "max_seq_length = 2048\n",
    "\n",
    "tuned_model = f\"{base_model.replace('/', '_')}-qlora-social-media\"\n",
    "\n",
    "tuned_model_top_dir = f\"model_{tuned_model}\"\n",
    "tuned_model_save_dir = f\"{tuned_model_top_dir}/model_save\"\n",
    "tuned_model_gguf_dir = f\"{tuned_model_top_dir}/model_gguf\"\n",
    "tuned_model_checkpoints_dir = f\"{tuned_model_top_dir}/checkpoints\"\n",
    "tuned_model_logs_dir = f\"{tuned_model_top_dir}/logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=tuned_model_save_dir,\n",
    "    # model_name=base_model,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    "    load_in_8bit=False,\n",
    "    full_finetuning=False,\n",
    "    random_state=42,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "alpaca_prompt = \"\"\"You are a social media user. Respond to the comment you are shown.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "BOS_TOKEN = tokenizer.bos_token\n",
    "\n",
    "inputs = tokenizer(\n",
    "    [\n",
    "        alpaca_prompt.format(\n",
    "            \"Tube amps are objectively the best amps.\",  # prompt\n",
    "            \"\",  # output - leave this blank for generation!\n",
    "        )\n",
    "    ],\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
