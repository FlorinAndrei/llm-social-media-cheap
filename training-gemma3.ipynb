{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastModel, to_sharegpt\n",
    "from unsloth.chat_templates import get_chat_template, standardize_data_formats, train_on_responses_only\n",
    "from datasets import load_dataset\n",
    "from transformers import TextStreamer\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model will OoM with 24 GB VRAM\n",
    "# base_model = \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\"\n",
    "# so use this one instead\n",
    "base_model = \"unsloth/gemma-3-27b-it-bnb-4bit\"\n",
    "max_tokens = 2048\n",
    "\n",
    "tuned_model = f\"{base_model.replace('/', '_')}-qlora-social-media\"\n",
    "\n",
    "tuned_model_top_dir = f\"model_{tuned_model}\"\n",
    "tuned_model_save_dir = f\"{tuned_model_top_dir}/model_save\"\n",
    "tuned_model_gguf_dir = f\"{tuned_model_top_dir}/model_gguf\"\n",
    "tuned_model_checkpoints_dir = f\"{tuned_model_top_dir}/checkpoints\"\n",
    "tuned_model_logs_dir = f\"{tuned_model_top_dir}/logs\"\n",
    "\n",
    "os.makedirs(tuned_model_top_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name=base_model,\n",
    "    max_seq_length=max_tokens,\n",
    "    load_in_4bit=True,\n",
    "    load_in_8bit=False,\n",
    "    full_finetuning=False,\n",
    "    # use_exact_model_name=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers=False,\n",
    "    finetune_language_layers=True,\n",
    "    finetune_attention_modules=True,\n",
    "    finetune_mlp_modules=True,\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    "    use_rslora=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_example = 4\n",
    "\n",
    "dataset = load_dataset(\"csv\", name=\"csv-for-gemma3\", split=\"train\", data_files=\"conversations.csv\")\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "print(dataset.column_names)\n",
    "print(json.dumps(dataset[n_example], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"gemma-3\",\n",
    ")\n",
    "\n",
    "dataset = to_sharegpt(\n",
    "    dataset,\n",
    "    merged_prompt=\"[[\\nYour input is:\\n{parent_text}]]\",\n",
    "    output_column_name=\"comment_body\",\n",
    ")\n",
    "print(json.dumps(dataset[n_example], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = standardize_data_formats(dataset)\n",
    "print(json.dumps(dataset[n_example], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chat_template(examples):\n",
    "    texts = tokenizer.apply_chat_template(examples[\"conversations\"])\n",
    "    return {\"text\": texts}\n",
    "\n",
    "\n",
    "dataset = dataset.map(apply_chat_template, batched=True)\n",
    "print(json.dumps(dataset[n_example], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(\"conversations\")\n",
    "print(json.dumps(dataset[n_example], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_length = len(dataset)\n",
    "batch_size = 1\n",
    "accum_steps = 2\n",
    "\n",
    "training_steps = dataset_length // (batch_size * accum_steps)\n",
    "warmup_steps = training_steps // 10\n",
    "\n",
    "print(f\"dataset length: {dataset_length}\")\n",
    "print(f\"training steps: {training_steps}\")\n",
    "print(f\"warmup steps:   {warmup_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=None,\n",
    "    dataset_num_proc=2,\n",
    "    args=SFTConfig(\n",
    "        dataset_text_field=\"text\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=accum_steps,\n",
    "        save_strategy=\"steps\",\n",
    "        logging_steps=1,\n",
    "        logging_strategy=\"steps\",\n",
    "        #\n",
    "        # full training run\n",
    "        #\n",
    "        num_train_epochs=1,\n",
    "        warmup_steps=warmup_steps,\n",
    "        save_steps=1000,\n",
    "        #\n",
    "        # short training run\n",
    "        #\n",
    "        # max_steps=100,\n",
    "        # warmup_steps=10,\n",
    "        # save_steps=10,\n",
    "        #\n",
    "        # end of training run length definition\n",
    "        learning_rate=1e-4,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=42,\n",
    "        output_dir=tuned_model_checkpoints_dir,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"<start_of_turn>user\\n\",\n",
    "    response_part=\"<start_of_turn>model\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(trainer.train_dataset[n_example][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    tokenizer.decode(\n",
    "        [tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[n_example][\"labels\"]]\n",
    "    ).replace(tokenizer.pad_token, \" \")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if any f\"outputs/checkpoint-*\" exists, resume from it\n",
    "if any(f.startswith(\"checkpoint-\") for f in os.listdir(tuned_model_checkpoints_dir)):\n",
    "    print(\"Resuming from checkpoint\")\n",
    "    trainer_stats = trainer.train(resume_from_checkpoint=True)\n",
    "else:\n",
    "    print(\"Starting from scratch\")\n",
    "    trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"gemma-3\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Give a simple yes or no answer: is water wet? Do not elaborate at all beyond the one-word answer.\",\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "_ = model.generate(\n",
    "    **tokenizer([text], return_tensors=\"pt\").to(\"cuda\"),\n",
    "    max_new_tokens=max_tokens,\n",
    "    # recommended Gemma-3 settings\n",
    "    temperature=1.0,\n",
    "    top_p=0.95,\n",
    "    min_p=0.0,\n",
    "    top_k=64,\n",
    "    streamer=TextStreamer(tokenizer, skip_prompt=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only the QLoRA weights are saved here, not the complete model\n",
    "# the base model shards are still required if you want to run the fine-tuned QLoRA weights\n",
    "model.save_pretrained(tuned_model_save_dir)\n",
    "tokenizer.save_pretrained(tuned_model_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge QLoRA weights with BF16 weights and save\n",
    "model.save_pretrained_merged(tuned_model_gguf_dir, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save merged weights in GGUF format\n",
    "\n",
    "# model.save_pretrained_gguf(tuned_model_gguf_dir, quantization_type=\"Q8_0\")\n",
    "# model.save_pretrained_gguf(tuned_model_gguf_dir, quantization_type=\"F16\")\n",
    "model.save_pretrained_gguf(tuned_model_gguf_dir, quantization_type=\"BF16\")\n",
    "# model.save_pretrained_gguf(tuned_model_gguf_dir, quantization_type=\"F32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate Ollama modelfile for GGUF\n",
    "# requires running tokenizer.apply_chat_template() earlier\n",
    "with open(f\"{tuned_model_gguf_dir}/Modelfile\", \"w\") as f:\n",
    "    f.write(tokenizer._ollama_modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(trainer_stats.metrics, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the fine-tuned model into Ollama\n",
    "\n",
    "https://www.gpu-mart.com/blog/import-models-from-huggingface-to-ollama\n",
    "\n",
    "This could be automated, but there are so many variations, and this only needs to be done once, after the model is fine-tuned and tested, I kept this as a manual step.\n",
    "\n",
    "```\n",
    "# cd into ${tuned_model_top_dir}\n",
    "cd model_unsloth_gemma-3-27b-it-bnb-4bit-qlora-social-media/\n",
    "\n",
    "# merge the shards (skip if only one shard exists)\n",
    "# you only need to indicate the first shard\n",
    "llama-gguf-split --merge model_gguf.BF16-00001-of-00002.gguf model_gguf.BF16.gguf\n",
    "\n",
    "# remove the shards\n",
    "rm model_gguf.BF16-*-of-*.gguf\n",
    "\n",
    "# quantize the model\n",
    "llama-quantize model_gguf.BF16.gguf model_gguf.Q4_K_M.gguf Q4_K_M\n",
    "\n",
    "# remove the merged BF16 GGUF\n",
    "rm model_gguf.BF16.gguf\n",
    "\n",
    "# copy the Modelfile out of model_gguf\n",
    "cp model_gguf/Modelfile .\n",
    "```\n",
    "\n",
    "Edit the Modelfile and replace the value (file location) in the top line with the actual file name of the quantized GGUF:\n",
    "\n",
    "```\n",
    "FROM \"./model_gguf.Q4_K_M.gguf\"\n",
    "```\n",
    "\n",
    "Also adjust the temperature, `num_predict` (max tokens), and/or other parameters, to match the values in the `model.generate()` step above.\n",
    "\n",
    "Load the model in Ollama. Choose a name that reflects both the base model, and the fine tuning you did:\n",
    "\n",
    "```\n",
    "ollama create \"gemma-3-27b-it-bnb-4bit-qlora-social-media\" -f Modelfile\n",
    "```\n",
    "\n",
    "Now you can run the model in Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
